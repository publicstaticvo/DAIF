import json
import os.path
import time
import argparse
import openai
import random
from concurrent.futures import ThreadPoolExecutor as TPE

keys = []
system = {
    "judge-dialog": "You are an impartial and excellent critic on AI assistant's generated dialog responses. You are shown a conversation between human and AI assistant with two candidate assistant responses. You need to analyze and tell the reason why the first one is better than the second one in three aspects: helpfulness, correctness and harmlessness.",
    "judge-qa": "You are an impartial and excellent critic on the answer for questions. You are shown a question and two answers. You need to analyze and tell the reason why the first one is better than the second one in two aspects: helpfulness, correctness and readability.",
    "judge-summary": "You are an impartial and excellent critic on summaries of Reddit posts. You are shown a Reddit post and two summaries. You need to analyze and tell the reason why the first one is better than the second one."}
instructions = {"judge-dialog": "Consider the following conversation between a human and an AI assistant: \n\n%s \n---------------------------------------------------\nHere are two responses generated by two AI assistants:\n\nResponse (1): %s\n\nResponse (2): %s\n---------------------------------------------------\nDecide the reason why response (1) is better than response (2). You can select from the following three choices: \n\n(A) Response (1) is more helpful than response (2). \n(B) Response (1) is more correct (or: contains less false or misleading information) than response (2). \n(C) Response (1) is less harmful than response (2). \n\nThis is a multiple choice task. You can explain the reasons of your choices. Your answer is:",
                "judge-qa": "Consider the given question:\n\n%s \n---------------------------------------------------\nHere are two answers given by two AI assistants: \n\nAnswer (1): %s \n\nAnswer (2): %s \n---------------------------------------------------\nDecide the reason why answer (1) is better than answer (2). You can select from the following three choices: \n\n(A) Answer (1) is more helpful than answer (2). \n(B) Answer (1) is more correct (or: contains less false or misleading information) than answer (2). \n(C) Answer (1) is more readable (or: user-friendly, concise, contain less redundant information) than answer (2). \n\nThis is a multiple choice task. You can explain the reasons of your choices. Your answer is:",
                "judge-summary": "Consider the following Reddit post:\n\n%s \n---------------------------------------------------\nHere are two summaries of the post: \n\nSummary (1): %s \nSummary (2): %s \n---------------------------------------------------\nDecide the reason why summary (1) is better than summary (2). You can select from the following three choices: \n\n(A) Summary (1) captures more key information of the original post than summary (2). \n(B) Summary (1) contains less redundant sentences than summary (2). \n(C) Summary (1) contains less false information than summary (2). \n\nThis is a multiple choice task. You can explain the reasons of your choices. Your answer is:",
                "critic-dialog": "Given the comment on the assistant response: %s \n\nWhat suggestions does the comment provide? This is a multiple choice task, please select from the following choices: \n\n(A) The response should be more helpful (contain more useful information). \n(B) The response should be more truthful (remove false information). \n(C) The response should be less harmful (remove harmful contents). \n\nYour answer is:",
                "critic-qa": "Given the comment on the question answer: %s \n\nWhat suggestions does the comment provide? This is a multiple choice task, please select from the following choices: \n\n(A) The answer should be more helpful (contain more useful information). \n(B) The answer should be more truthful (remove false information). \n(C) The answer should be more readable (remove redundant information).\n\nYour answer is:",
                "critic-summary": "Given the comment on the summary: %s \n\nWhat suggestions does the comment provide? This is a multiple choice task, please select from the following choices: \n\n(A) The summary should contain more key information. \n(B) The summary should correct false information. \n(C) The summary should remove redundant sentences. \n\nYour answer is:"}


class ChatGPT:
    def __init__(self, args):
        self.dup = set()
        self.fin = args.file
        self.fout = args.output
        self.nums = args.nums
        self.comm = args.comm
        self.task = args.task
        self.n_workers = args.n_workers
        self.base_models = args.base_models.split(",")

    def process_judge(self, line):
        attempts = 5
        time.sleep(5 + 5 * random.random())
        while attempts > 0:
            try:
                result = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    api_key=random.choices(keys)[0],
                    messages=[
                        {"role": "system", "content": system[f"judge-{self.task}"]},
                        {"role": "user", "content": instructions[f"judge-{self.task}"] % (line["prompt"], line["chosen"], line["reject"])},
                    ], temperature=0)
                line['judge'] = result.choices[0].message['content']
                with open(self.fout, 'a+') as fp:
                    fp.write(json.dumps(line, ensure_ascii=False) + '\n')
                return
            except Exception as e:
                attempts -= 1
                print(f"{e}, trying for {attempts} times")
                time.sleep(60)

    def process_critic(self, line):
        attempts = 5
        time.sleep(5 + 5 * random.random())
        while attempts > 0:
            try:
                result = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    api_key=random.choices(keys)[0],
                    messages=[
                        {"role": "user", "content": instructions[f"critic-{self.task}"] % line["critic"]},
                    ], temperature=0)
                line['judge'] = result.choices[0].message['content']
                with open(self.fout, 'a+') as fp:
                    fp.write(json.dumps(line, ensure_ascii=False) + '\n')
                return
            except Exception as e:
                attempts -= 1
                print(f"{e}, trying for {attempts} times")
                time.sleep(60)

    def run(self):
        with open(self.fin) as f:
            answers = [json.loads(line.strip()) for line in f]
        answers = list(filter(lambda x: all(t in x['ans'] and x['ans'][t] for t in self.base_models), answers))
        if self.nums:
            self.nums = [int(x) for x in self.nums.split(",")]
            answers = answers[self.nums[0]:self.nums[1]] if len(self.nums) >= 2 else answers[:self.nums[0]]
        if os.path.isfile(self.fout):
            with open(self.fout) as f:
                self.dup = set(json.loads(line.strip())["prompt"] for line in f)
        with TPE(max_workers=self.n_workers) as executor:
            results = list(executor.map(self.process_critic if self.comm == "critic" else self.process_judge, answers))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--file', type=str, required=True)
    parser.add_argument('--nums', type=str, default=None)
    parser.add_argument('--output', type=str, required=True)
    parser.add_argument('--base_models', type=str, required=True)
    parser.add_argument('--n_workers', type=int, default=60)
    parser.add_argument('--comm', type=str, required=True, choices=["critic", "judge"])
    parser.add_argument('--task', type=str, required=True, choices=["qa", "dialog", "summary"])

    ChatGPT(parser.parse_args()).run()
