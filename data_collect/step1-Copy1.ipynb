{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc9feb1-d90e-4bdd-a400-e901d5d4bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import datasets\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "def download_dataset(data, subset=None, split=None):\n",
    "    success = True\n",
    "    while success:\n",
    "        try:\n",
    "            d = datasets.load_dataset(data, name=subset, split=split)\n",
    "            success = False\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    return d\n",
    "\n",
    "def load_local(fn):\n",
    "    return [json.loads(line.strip()) for line in open(fn)]\n",
    "\n",
    "def print_json(d, w):\n",
    "    with open(w, \"w+\") as f:\n",
    "        f.write(\"\\n\".join([json.dumps(i, ensure_ascii=False) for i in d]))\n",
    "\n",
    "def get_webgpt(d):\n",
    "    d = d[\"train\"]\n",
    "    data = []\n",
    "    for item in d:\n",
    "        if float(item['score_0']) == float(item['score_1']):\n",
    "            continue\n",
    "        elif float(item['score_0']) < float(item['score_1']):\n",
    "            chosen, reject = item['answer_1'], item['answer_0']\n",
    "        else:\n",
    "            chosen, reject = item['answer_0'], item['answer_1']\n",
    "        chosen = re.sub(r\" [\\(\\[].*?[\\)\\]]\", \"\", chosen)\n",
    "        chosen = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", chosen)\n",
    "        reject = re.sub(r\" [\\(\\[].*?[\\)\\]]\", \"\", reject)\n",
    "        reject = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", reject)\n",
    "        data.append({\"prompt\": \"Human: \" + item['question']['full_text'].strip() + \"\\n\\nAssistant:\", \"chosen\": chosen.strip(), \"rejected\": reject.strip()})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1015570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201126 19368 116667\n",
      "201110 19354 116657\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1: Processing data\n",
    "from transformers import LlamaTokenizer as LT\n",
    "t = LT.from_pretrained('/mnt/ewwe/yts/llm/models/lmsys-vicuna-7b-v1.1')\n",
    "all_problems = set()\n",
    "\n",
    "def fi(x):\n",
    "    if x['prompt'] not in all_problems:\n",
    "        if len(t.encode(x['prompt'])) <= 1024:\n",
    "            all_problems.add(x['prompt'])\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "per_task = 10000\n",
    "fn = \"/mnt/ewwe/yts/llm/daif/data_collect/data\"\n",
    "dialog = load_local(f'{fn}/origin/hhrlhf.txt')  # dialog = download_dataset(\"Dahoas/rl-prompt-dataset\", split=\"train\")\n",
    "qa = load_local(f'{fn}/origin/webgpt.txt')  # qa = get_webgpt(download_dataset(\"openai/webgpt_comparisons\", split=\"train\"))\n",
    "summary = load_local(f'{fn}/origin/summarize_tldr.txt')  # dialog = download_dataset(\"CarperAI/openai_summarize_tldr\", split=\"train\")\n",
    "# You can also visit f'{fn}/origin' for training data.\n",
    "print(len(dialog), len(qa), len(summary))\n",
    "dialog = [x for x in dialog if fi(x)]\n",
    "qa = [x for x in qa if fi(x)]\n",
    "summary = [x for x in summary if fi(x)]\n",
    "print(len(dialog), len(qa), len(summary))\n",
    "problems = random.sample(dialog, per_task) + random.sample(qa, per_task) + random.sample(summary, per_task)\n",
    "print_json(problems, f'{fn}/origin/problems.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7ed0c87-546d-4328-bd97-0ecae0f3d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_local(f'{fn}/origin/problems.txt')\n",
    "prefix = \"Please summarize the following Reddit post from the first perspective in no more than 100 words.\"\n",
    "prefix2 = \"Please summarize the following Reddit post in no more than 100 words.\"\n",
    "for x in d:\n",
    "    x['prompt'] = x['prompt'].replace(prefix, prefix2)\n",
    "print_json(d, f'{fn}/origin/problems.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b9827a6-68aa-4ca9-8b4e-4f078bb0985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009\n"
     ]
    }
   ],
   "source": [
    "d = load_local(f'{fn}/origin/problems.txt')\n",
    "ml = 0\n",
    "for x in d:\n",
    "    ml = max(ml, len(t.encode(x[\"prompt\"])))\n",
    "print(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: Grouping\n",
    "model1 = 'vicuna-7b'\n",
    "model2 = 'vicuna-13b'\n",
    "d = [x for i in range(8) for x in load_local(f\"{fn}/initial_answers/ppl-{model1}-{i}.txt\")]\n",
    "d = [x for x in d if \"Fact-check\" not in x[\"prompt\"]]\n",
    "l = len(d)\n",
    "\n",
    "d = sorted(d, key=lambda x: x['loss'])\n",
    "easy, mid, hard = d[:l//3], d[l//3:2*l//3], d[2*l//3:]\n",
    "\n",
    "d13 = [x for i in range(8) for x in load_local(f\"{fn}/initial_answers/{model2}-{i}.txt\")]\n",
    "another_answer = {i['prompt']: i['output'] for i in d13}\n",
    "for i in range(len(easy)):\n",
    "    easy[i]['difficulty'] = 'easy'\n",
    "    easy[i]['feedback_type'] = 'critic'\n",
    "    easy[i]['output'] = {model1: easy[i]['output'], model2: another_answer[easy[i]['prompt']]}\n",
    "for i in range(len(mid)):\n",
    "    mid[i]['difficulty'] = 'medium'\n",
    "    mid[i]['feedback_type'] = 'refine'\n",
    "    mid[i]['output'] = {model1: mid[i]['output'], model2: another_answer[mid[i]['prompt']]}\n",
    "for i in range(len(hard)):\n",
    "    hard[i]['difficulty'] = 'hard'\n",
    "    hard[i]['feedback_type'] = 'prefer'\n",
    "    hard[i]['output'] = {model1: hard[i]['output'], model2: another_answer[hard[i]['prompt']]}\n",
    "print_json(easy + mid + hard, f\"{fn}/initial_answers/group.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1908144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.4: Collect Raw Feedbacks\n",
    "# 1.4.1 Collect Raw Feedbacks\n",
    "critic, refine, prefer = [], [], []\n",
    "with open(f\"{fn}/data/feedback/feedbacks.txt\") as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line.strip())\n",
    "        fb = line[\"feedback\"]\n",
    "        if line['feedback_type'] == 'critic':\n",
    "            critic.append(line)\n",
    "        elif line['feedback_type'] == 'refine':\n",
    "            element = \"answer\" if line['task'] == \"qa\" else (\"response\" if line['task'] == \"dialog\" else \"summary\")\n",
    "            trigger = f\"A better {element} is:\"\n",
    "            if trigger in fb:\n",
    "                c = fb[fb.index(trigger) + len(trigger):].strip()\n",
    "                line['chosen'] = c\n",
    "                line['reject'] = line['output'][model1]\n",
    "                del line['output'], line['prompt-api']\n",
    "                refine.append(line)\n",
    "        else:\n",
    "            a = \"A)\" in fb or \"A -\" in fb or fb in [\"A\", \"A.\"]\n",
    "            b = \"B)\" in fb or \"B -\" in fb or fb in [\"B\", \"B.\"]\n",
    "            c = \"C)\" in fb or \"C -\" in fb or fb in [\"C\", \"C.\"]\n",
    "            if c:\n",
    "                continue\n",
    "            elif a and not b:\n",
    "                line['chosen'] = line['output'][model1]\n",
    "                line['reject'] = line['output'][model2]\n",
    "                del line['output'], line['prompt-api']\n",
    "                prefer.append(line)\n",
    "            elif b and not a:\n",
    "                line['chosen'] = line['output'][model2]\n",
    "                line['reject'] = line['output'][model1]\n",
    "                del line['output'], line['prompt-api']\n",
    "                prefer.append(line)\n",
    "print_json(refine, f'{fn}/feedback/refinement.txt')\n",
    "print_json(prefer, f'{fn}/feedback/preference.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4.2 Construct dataset for step 1.5\n",
    "hd = [\"Below is a conversation between a human and an AI assistant. Given the comment on the assistant's last response (started with \\\"Comment: \\\"), please rewrite the response according to the advice of improvement presented in the comment to make it more helpful, truthful and less harmful to the human. \\n\\n\\n\",\n",
    "\"\\n\\n\\nResponse: \",\n",
    "\"\\n\\n\\nComment: \",\n",
    "\"\\n\\n\\nRevised response: \"]\n",
    "hq = [\"Below is a question and its intended answer. Given the comment on the answer (started with \\\"Comment: \\\"), please rewrite the answer according to the advice of improvement presented in the comment to make it more correct, clear and readable to the human. \\n\\n\\n\",\n",
    "\"\\n\\n\\nAnswer: \",\n",
    "\"\\n\\n\\nComment: \",\n",
    "\"\\n\\n\\nRevised answer: \"]\n",
    "hs = [\"Below is a Reddit post and its intended summary. Given the comment on the summary (started with \\\"Comment: \\\"), please rewrite the summary according to the advice of improvement presented in the comment to make it more accurate and brief for others to read. \\n\\n\\n\",\n",
    "\"\\n\\n\\nSummary: \",\n",
    "\"\\n\\n\\nComment: \",\n",
    "\"\\n\\n\\nRevised summary: \"]\n",
    "\n",
    "for c in critic:\n",
    "    c['origin_prompt'] = c['prompt']\n",
    "    c['reject'] = c['output'][model1]\n",
    "    if c['task'] == 'dialog':\n",
    "        c['prompt'] = hd[0] + c[\"prompt-api\"] + hd[1] + c[\"reject\"] + hd[2] + c[\"feedback\"] + hd[3]\n",
    "    elif c['task'] == 'qa':\n",
    "        c['prompt'] = hq[0] + c[\"prompt-api\"] + hq[1] + c[\"reject\"] + hq[2] + c[\"feedback\"] + hq[3]\n",
    "    else:\n",
    "        c['prompt'] = hs[0] + c[\"prompt-api\"] + hs[1] + c[\"reject\"] + hs[2] + c[\"feedback\"] + hs[3]\n",
    "print_json(critic, f'{fn}/feedback/critique.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ecbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.6: Set up Train and Valid Datasets for RLHF-RM and DPO\n",
    "import re\n",
    "remove = re.compile(r\"(T|t)he (original |revised )?(answer|response|summary)|Comment:|Human:|Revised answer:|Explanation:|To provide more cont\")\n",
    "c = [x for i in range(8) for x in load_local(f\"{fn}/feedback/improve-{i}.txt\")]\n",
    "critic = []\n",
    "for x in c:\n",
    "    x['prompt'] = x['origin_prompt']\n",
    "    answer = x['output'].strip().split(\"\\n\\n\")\n",
    "    if answer[0][:6] == \"Human:\":\n",
    "        answer = answer[1:]\n",
    "    cut = -1\n",
    "    for j, y in enumerate(answer):\n",
    "        if re.findall(remove, y):\n",
    "            cut = j\n",
    "            break\n",
    "    answer = \"\\n\\n\".join(answer[:cut] if cut >= 0 else answer)\n",
    "    if not answer or answer == x['reject']:\n",
    "        continue\n",
    "    x['chosen'] = answer\n",
    "    del x['origin_prompt'], x['output'], x['prompt-api']\n",
    "    critic.append(x)\n",
    "    \n",
    "refine = load_local(f'{fn}/feedback/refinement.txt')\n",
    "prefer = load_local(f'{fn}/feedback/preference.txt')\n",
    "valid_size = [0.1 * len(critic), 0.1 * len(refine), 0.1 * len(prefer)]\n",
    "random.shuffle(critic)\n",
    "random.shuffle(refine)\n",
    "random.shuffle(prefer)\n",
    "print_json(critic[:valid_size[0]] + refine[:valid_size[1]] + prefer[:valid_size[2]], f'{fn}/feedback/valid.txt')\n",
    "print_json(critic[valid_size[0]:] + refine[valid_size[1]:] + prefer[valid_size[2]:], f'{fn}/feedback/train.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
