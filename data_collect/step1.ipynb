{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc9feb1-d90e-4bdd-a400-e901d5d4bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import datasets\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "def download_dataset(data, subset=None, split=None):\n",
    "    success = True\n",
    "    while success:\n",
    "        try:\n",
    "            d = datasets.load_dataset(data, name=subset, split=split)\n",
    "            success = False\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    return d\n",
    "\n",
    "def load_local(fn):\n",
    "    return [json.loads(line.strip()) for line in open(fn)]\n",
    "\n",
    "def print_json(d, w):\n",
    "    with open(w, \"w+\") as f:\n",
    "        f.write(\"\\n\".join([json.dumps(i, ensure_ascii=False) for i in d]))\n",
    "\n",
    "def get_webgpt(d):\n",
    "    d = d[\"train\"]\n",
    "    data = []\n",
    "    for item in d:\n",
    "        if float(item['score_0']) == float(item['score_1']):\n",
    "            continue\n",
    "        elif float(item['score_0']) < float(item['score_1']):\n",
    "            chosen, reject = item['answer_1'], item['answer_0']\n",
    "        else:\n",
    "            chosen, reject = item['answer_0'], item['answer_1']\n",
    "        chosen = re.sub(r\" [\\(\\[].*?[\\)\\]]\", \"\", chosen)\n",
    "        chosen = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", chosen)\n",
    "        reject = re.sub(r\" [\\(\\[].*?[\\)\\]]\", \"\", reject)\n",
    "        reject = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", reject)\n",
    "        data.append({\"prompt\": \"Human: \" + item['question']['full_text'].strip() + \"\\n\\nAssistant:\", \"chosen\": chosen.strip(), \"rejected\": reject.strip()})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b232369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/ewwe/.cache/huggingface/modules/datasets_modules/datasets/openai--summarize_from_feedback/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0 (last modified on Mon Feb  5 06:59:45 2024) since it couldn't be found locally at openai/summarize_from_feedback, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "d = download_dataset(\"openai/summarize_from_feedback\", subset='comparisons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4847c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {k: [] for k in d}\n",
    "for k in d:\n",
    "    for y in d[k]:\n",
    "        try:\n",
    "            sub = y['info']['subreddit'].strip()\n",
    "            title = y['info']['title'].strip()\n",
    "            post = y['info']['post'].strip()\n",
    "            if y['choice'] == 0:\n",
    "                chosen = y['summaries'][0]['text'].strip()\n",
    "                reject = y['summaries'][1]['text'].strip()\n",
    "            else:\n",
    "                chosen = y['summaries'][1]['text'].strip()\n",
    "                reject = y['summaries'][0]['text'].strip()\n",
    "            if sub != \"\" and title != \"\" and post != \"\" and chosen != \"\" and reject != \"\":\n",
    "                x[k].append({\"prompt\": f\"Human: Please summarize the following Reddit post in no more than 100 words: \\n\\nSUBREDDIT: {sub} \\n\\nTITLE: {title} \\n\\nPOST: {post} \\n\\nAssistant: \", \"chosen\": chosen, \"reject\": reject, \"task\": \"summary\"})\n",
    "        except:\n",
    "            pass\n",
    "for k in x:\n",
    "    print_json(x[k], f\"{k}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e888b759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86086\n"
     ]
    }
   ],
   "source": [
    "print(len(d['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Processing data\n",
    "per_task = 10000\n",
    "fn = \"data\"\n",
    "dialog = load_local(f'{fn}/origin/hhrlhf.txt')  # dialog = download_dataset(\"Dahoas/rl-prompt-dataset\", split=\"train\")\n",
    "qa = load_local(f'{fn}/origin/webgpt.txt')  # qa = get_webgpt(download_dataset(\"openai/webgpt_comparisons\", split=\"train\"))\n",
    "summary = load_local(f'{fn}/origin/summarize_tldr.txt')  # dialog = download_dataset(\"CarperAI/openai_summarize_tldr\", split=\"train\")\n",
    "# You can also visit f'{fn}/origin' for training data.\n",
    "problems = random.sample(dialog, per_task) + random.sample(qa, per_task) + random.sample(summary, per_task)\n",
    "print_json(problems, f'{fn}/origin/problems.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: Grouping\n",
    "model1 = 'vicuna-7b'\n",
    "model2 = 'vicuna-13b'\n",
    "d = [x for i in range(8) for x in load_local(f\"{fn}/initial_answers/ppl-{model1}-{i}.txt\")]\n",
    "d = [x for x in d if \"Fact-check\" not in x[\"prompt\"]]\n",
    "l = len(d)\n",
    "\n",
    "d = sorted(d, key=lambda x: x['loss'])\n",
    "easy, mid, hard = d[:l//3], d[l//3:2*l//3], d[2*l//3:]\n",
    "\n",
    "d13 = [x for i in range(8) for x in load_local(f\"{fn}/initial_answers/{model2}-{i}.txt\")]\n",
    "another_answer = {i['prompt']: i['output'] for i in d13}\n",
    "for i in range(len(easy)):\n",
    "    easy[i]['difficulty'] = 'easy'\n",
    "    easy[i]['feedback_type'] = 'critic'\n",
    "    easy[i]['output'] = {model1: easy[i]['output'], model2: another_answer[easy[i]['prompt']]}\n",
    "for i in range(len(mid)):\n",
    "    mid[i]['difficulty'] = 'medium'\n",
    "    mid[i]['feedback_type'] = 'refine'\n",
    "    mid[i]['output'] = {model1: mid[i]['output'], model2: another_answer[mid[i]['prompt']]}\n",
    "for i in range(len(hard)):\n",
    "    hard[i]['difficulty'] = 'hard'\n",
    "    hard[i]['feedback_type'] = 'prefer'\n",
    "    hard[i]['output'] = {model1: hard[i]['output'], model2: another_answer[hard[i]['prompt']]}\n",
    "print_json(easy + mid + hard, f\"{fn}/initial_answers/group.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1908144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.4: Collect Raw Feedbacks\n",
    "# 1.4.1 Collect Raw Feedbacks\n",
    "critic, refine, prefer = [], [], []\n",
    "with open(f\"{fn}/data/feedback/feedbacks.txt\") as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line.strip())\n",
    "        fb = line[\"feedback\"]\n",
    "        \n",
    "        if line['feedback_type'] == 'critic':\n",
    "            critic.append(line)\n",
    "        elif line['feedback_type'] == 'refine':\n",
    "            element = \"answer\" if line['task'] == \"qa\" else (\"response\" if line['task'] == \"dialog\" else \"summary\")\n",
    "            trigger = f\"A better {element} is:\"\n",
    "            if trigger in fb:\n",
    "                c = fb[fb.index(trigger) + len(trigger):].strip()\n",
    "                line['chosen'] = c\n",
    "                line['reject'] = line['output'][model1]\n",
    "                del line['output'], line['prompt-api']\n",
    "                refine.append(line)\n",
    "        else:\n",
    "            a = \"A)\" in fb or \"A -\" in fb or fb in [\"A\", \"A.\"]\n",
    "            b = \"B)\" in fb or \"B -\" in fb or fb in [\"B\", \"B.\"]\n",
    "            c = \"C)\" in fb or \"C -\" in fb or fb in [\"C\", \"C.\"]\n",
    "            if c:\n",
    "                continue\n",
    "            elif a and not b:\n",
    "                line['chosen'] = line['output'][model1]\n",
    "                line['reject'] = line['output'][model2]\n",
    "                del line['output'], line['prompt-api']\n",
    "                prefer.append(line)\n",
    "            elif b and not a:\n",
    "                line['chosen'] = line['output'][model2]\n",
    "                line['reject'] = line['output'][model1]\n",
    "                del line['output'], line['prompt-api']\n",
    "                prefer.append(line)\n",
    "print_json(refine, f'{fn}/feedback/refinement.txt')\n",
    "print_json(prefer, f'{fn}/feedback/preference.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4.2 Construct dataset for step 1.5\n",
    "hd = [\"Below is a conversation between a human and an AI assistant. Given the comment on the assistant's last response (started with \\\"Comment: \\\"), please rewrite the response according to the advice of improvement presented in the comment to make it more helpful, truthful and less harmful to the human. \\n\\n\\n\",\n",
    "\"\\n\\n\\nResponse: \",\n",
    "\"\\n\\n\\nComment: \",\n",
    "\"\\n\\n\\nRevised response: \"]\n",
    "hq = [\"Below is a question and its intended answer. Given the comment on the answer (started with \\\"Comment: \\\"), please rewrite the answer according to the advice of improvement presented in the comment to make it more correct, clear and readable to the human. \\n\\n\\n\",\n",
    "\"\\n\\n\\nAnswer: \",\n",
    "\"\\n\\n\\nComment: \",\n",
    "\"\\n\\n\\nRevised answer: \"]\n",
    "hs = [\"Below is a Reddit post and its intended summary. Given the comment on the summary (started with \\\"Comment: \\\"), please rewrite the summary according to the advice of improvement presented in the comment to make it more accurate and brief for others to read. \\n\\n\\n\",\n",
    "\"\\n\\n\\nSummary: \",\n",
    "\"\\n\\n\\nComment: \",\n",
    "\"\\n\\n\\nRevised summary: \"]\n",
    "\n",
    "for c in critic:\n",
    "    c['origin_prompt'] = c['prompt']\n",
    "    c['reject'] = c['output'][model1]\n",
    "    if c['task'] == 'dialog':\n",
    "        c['prompt'] = hd[0] + c[\"prompt-api\"] + hd[1] + c[\"reject\"] + hd[2] + c[\"feedback\"] + hd[3]\n",
    "    elif c['task'] == 'qa':\n",
    "        c['prompt'] = hq[0] + c[\"prompt-api\"] + hq[1] + c[\"reject\"] + hq[2] + c[\"feedback\"] + hq[3]\n",
    "    else:\n",
    "        c['prompt'] = hs[0] + c[\"prompt-api\"] + hs[1] + c[\"reject\"] + hs[2] + c[\"feedback\"] + hs[3]\n",
    "print_json(critic, f'{fn}/feedback/critique.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ecbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.6: Set up Train and Valid Datasets for RLHF-RM and DPO\n",
    "import re\n",
    "remove = re.compile(r\"(T|t)he (original |revised )?(answer|response|summary)|Comment:|Human:|Revised answer:|Explanation:|To provide more cont\")\n",
    "c = [x for i in range(8) for x in load_local(f\"{fn}/feedback/improve-{i}.txt\")]\n",
    "critic = []\n",
    "for x in c:\n",
    "    x['prompt'] = x['origin_prompt']\n",
    "    answer = x['output'].strip().split(\"\\n\\n\")\n",
    "    if answer[0][:6] == \"Human:\":\n",
    "        answer = answer[1:]\n",
    "    cut = -1\n",
    "    for j, y in enumerate(answer):\n",
    "        if re.findall(remove, y):\n",
    "            cut = j\n",
    "            break\n",
    "    answer = \"\\n\\n\".join(answer[:cut] if cut >= 0 else answer)\n",
    "    if not answer or answer == x['reject']:\n",
    "        continue\n",
    "    x['chosen'] = answer\n",
    "    del x['origin_prompt'], x['output'], x['prompt-api']\n",
    "    critic.append(x)\n",
    "    \n",
    "refine = load_local(f'{fn}/feedback/refinement.txt')\n",
    "prefer = load_local(f'{fn}/feedback/preference.txt')\n",
    "valid_size = [0.1 * len(critic), 0.1 * len(refine), 0.1 * len(prefer)]\n",
    "random.shuffle(critic)\n",
    "random.shuffle(refine)\n",
    "random.shuffle(prefer)\n",
    "print_json(critic[:valid_size[0]] + refine[:valid_size[1]] + prefer[:valid_size[2]], f'{fn}/feedback/valid.txt')\n",
    "print_json(critic[valid_size[0]:] + refine[valid_size[1]:] + prefer[valid_size[2]:], f'{fn}/feedback/train.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
